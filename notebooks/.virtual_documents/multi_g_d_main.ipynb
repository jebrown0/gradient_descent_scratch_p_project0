








import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Implementation of explicit dtype hinting
dtype_map = {'hours_studied': 'float32', 'previous_scores': 'float32',
             'extra_activities': 'category', 'sleep_hours': 'float32',
             'sample_question': 'float32', 'performance_index': 'float32'}


column_names = ['hours_studied', 'previous_scores', 'extra_activities',
                'sleep_hours', 'sample_question', 'performance_index']

# While the Uber_Fares dataset is massive, I am willing to sacrifice performance to improve accuracy
sp_df = pd.read_csv('../data/dataset_a_s_e_p/Student_Performance/Student_Performance.csv', names=column_names, header=0, dtype=dtype_map)

# Data exploration
print("Print first 20 rows of each column:\n", sp_df.head(20))

print("\nData description:\n")
with pd.option_context('display.max_columns', None, 'display.max_rows', None):
    print(sp_df.describe())





one_hot = np.eye(sp_df['extra_activities'].nunique(), dtype=np.float32)[sp_df['extra_activities'].cat.codes]
X_num = sp_df.drop(columns=['extra_activities']).to_numpy(dtype=np.float32)
X = np.concatenate([X_num, one_hot], axis=1)

print(sp_df.head(20))

print("\nData description:\n")
with pd.option_context('display.max_columns', None, 'display.max_rows', None):
    print(sp_df.describe())





numeric_columns = ['hours_studied', 'previous_scores', 'sleep_hours', 
                      'sample_question'] # Purposely skips 'extra_activities' as it is already scaled. 
                                        # 'performance_index' isn't scaled as it's the target column.

# Mean/STD calculation
mean = sp_df[numeric_columns].mean()
std = sp_df[numeric_columns].std()

# Standardization of each column
sp_df[numeric_columns] = ((sp_df[numeric_columns] - mean) / std)





# 80% split index
split_index = int(len(sp_df) * 0.8)

# Split of dataframe
train_sp_df = sp_df.iloc[:split_index]
test_sp_df = sp_df.iloc[split_index:]

# Size verification
print(f'Size of Training Set: {len(train_sp_df)}')
print(f'Size of Test Set: {len(test_sp_df)}')


# Data exploration
print("Print first 20 rows of each column:\n", train_sp_df.head(20))

print("\nData description:\n")
with pd.option_context('display.max_columns', None, 'display.max_rows', None):
    print(train_sp_df.describe())

print("\nData description:\n")
with pd.option_context('display.max_columns', None, 'display.max_rows', None):
    print(test_sp_df.describe())





# Check for missing data
print("\n Null values: \n", train_sp_df.isnull().sum())

# 100% validity verified
train_sp_df_clean = train_sp_df.copy()


# Data exploration/prep of test data
print("\n Null values: \n", test_sp_df.isnull().sum()) # Does not verifies 100% data validity.

# 100% validity verified
test_sp_df_clean = test_sp_df.copy()














# Feature columns to ensure only features are normalized
feature_columns = ['hours_studied', 'previous_scores', 'sleep_hours', 'sample_question', 'extra_activities']
# If algorithm performance is not adequeate 'passenger_count' will be removed

print(feature_columns)








# Prediction function/formula
def predict(X, b, w):
    # X: shape (n_samples, n_features)
    # w: shape (n_features,)
    # b: scalar
    return X.dot(w) + b











# Set-up for learning model
# Example: create random (dummy) data for demo purposes
np.random.seed(42)          # .seed() ensures that the random numbers generated are the same each time the program is run
n_samples = 100
n_features = 5
X_demo = np.random.randn(n_samples, n_features)    # Features matrix
weights_true = np.array([1.5, -2.0, 1.0, 1.5, 2])            # True weights (for testing)
y_demo = X_demo @ weights_true + 0.5             # y = Xw + b, with true bias 0.5


# Verify shape of X
print(X_demo.shape)


def g_d_func(X, y, b_init=0.0, learning_rate=0.05, iterations=1000, w_init=None):
    # Make sure w matches the number of features
    X = np.asarray(X, dtype=np.float32)               # shape (m, n)
    y = np.asarray(y, dtype=np.float32)               # shape (m,)
    n_features = X.shape[1]

    if w_init is None:
        w = np.zeros(n_features, dtype=np.float32)    # (n,)
    else:
        w = np.asarray(w_init, dtype=np.float32)
        assert w.shape == (n_features,), "w_init length must equal n_features"
    
    # Ensures b is able to move around the horizontal plane
    b = float(b_init)
    total_cost = []

    m = len(y)
    for i in range(iterations):
        y_hat = X.dot(w) + b
        error = y_hat - y

        # Compute gradients
        grad_w = (2 / len(X)) * X.T.dot(error)
        grad_b = (2 / len(X)) * error.sum() # Scalar

        # Update parameters
        w -= learning_rate * grad_w
        b -= learning_rate * grad_b

        # Track cost
        mse = (error ** 2).mean()
        total_cost.append(mse)

        if i % 100 == 0:
            print(f"Iter {i:5d}: MSE={mse:.4f}")
    
    print("Final Weights:", w)
    print("Final Bias:", b)
    print("Final Cost:", mse)

    return b, w, total_cost


demo_b, demo_w, demo_total_cost = g_d_func(X=X_demo, y=y_demo, learning_rate=0.01, iterations=410)





sns.lineplot(demo_total_cost)
plt.title('Demo: Cost Over Iterations')
plt.xlabel('Iterations')
plt.ylabel('Cost')














train_feature_data = train_sp_df_clean[feature_columns]
train_target_data = train_sp_df_clean['performance_index']

X_train = train_feature_data.to_numpy()
y_train = train_target_data.to_numpy()



test_feature_data = train_sp_df_clean[feature_columns]
test_target_data = train_sp_df_clean['performance_index']

X_test = test_feature_data.to_numpy()
y_test = test_target_data.to_numpy()


training_b, training_w, training_total_cost = g_d_func(X=X_train, y=y_train, learning_rate=0.2005, iterations=300000)











sns.lineplot(training_total_cost)
plt.title('Training Data: Cost Over Iterations')
plt.xlabel('Iterations')
plt.ylabel('Cost')








# Use predict function to test final 'w' and 'b' values
y_pred_test = predict(X_test, training_w, training_b)

# Calculate MSE (Mean Squared Error)
mse = np.mean((y_test - y_pred_test) ** 2)

# Calculate RMSE (Root Mean Squared Error)
rmse = np.sqrt(mse) # Puts MSE back into the root unit. Increases readablility.

print(f'Test RMSE: {rmse:.4f}')














# In order to implement LASSO Regression from scratch, I will be creating a new function that is specifically designed around the advancement.

def lasso_batch_gradient_descent(X, y, learning_rate=0.01, b=0.0, lambda_=0.1, iterations=1000):
    n_samples, n_features = X.shape
    w = np.random.randn(n_features) * 0.01
    cost_history = []
    
    
    for i_1 in range(iterations):
        y_pred = X.dot(w) + b
        residual = y - y_pred

        # Gradients
        dw = (-1/n_samples) * X.T.dot(residual) + lambda_ * np.sign(w)
        db = (-1/n_samples) * np.sum(residual)

        # Update rules
        w -= learning_rate * dw
        b -= learning_rate * db

        # Calculate cost
        cost = (1 / (2 * n_samples) * np.sum(residual ** 2) + lambda_ * np.sum(np.abs(w)))
        cost_history.append(cost)
        
        # Print progress every 10 iterations
        if i_1 % 10 == 0:
            print(f"Iteration: {i_1}: Cost={cost:.4f}")
    
    print("Final Weights:", w)
    print("Final Bias:", b)
    print("Final Cost:", cost)

    return w, b, cost_history


# Calling lasso_batch_gradient_descent with training data
training_w_lasso, training_b_lasso, training_cost_history_lasso = lasso_batch_gradient_descent(
    X_train, y_train, learning_rate=0.2999, lambda_=.05, iterations=1000)





sns.lineplot(training_cost_history_lasso)
plt.title('Training Data: Cost Over Iterations')
plt.xlabel('Iterations')
plt.ylabel('Cost')


y_hat = predict(X_test, training_b, training_w)
rmse  = np.sqrt(np.mean((y_test - y_hat)**2))
mae   = np.mean(np.abs(y_test - y_hat))
r2    = 1 - np.sum((y_test - y_hat)**2) / np.sum((y_test - y_test.mean())**2)
print(f"→ RMSE {rmse:,.4f} | MAE {mae:,.4f} | R² {r2:.4f}")


import matplotlib.pyplot as plt
plt.scatter(y_test, y_test - y_pred_test, alpha=0.4)
plt.axhline(0, color="k")
plt.xlabel("True y")
plt.ylabel("Residual (y_true − y_pred)")
plt.title("Residual plot — test set")
plt.show()






